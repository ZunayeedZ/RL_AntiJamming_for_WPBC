{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cba86602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt #for plotting functions\n",
    "from scipy.special import erfc\n",
    "from numpy import sum,isrealobj,sqrt\n",
    "from numpy.random import standard_normal\n",
    "from scipy.special import erfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf5ed22",
   "metadata": {},
   "source": [
    "### Required Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa988d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BPSK Modulation\n",
    "def bpsk_mod(mu,L):\n",
    "    from scipy.signal import upfirdn\n",
    "    s_bb = upfirdn(h=[1]*L, x=2*mu-1, up = L) # NRZ encoder; upfirdn: Upsample, FIR filter, and downsample.\n",
    "    t=np.arange(start = 0,stop = len(mu)*L) #discrete time base\n",
    "    return (s_bb,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff42e0",
   "metadata": {},
   "source": [
    "### More Required Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f794bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pathloss Model - Log distance\n",
    "def logNormalShadowing(Pt_dBm,Gt_dBi,Gr_dBi,f,d0,d,L,sigma,n):\n",
    "    #Pt_dBm = Transmitted power in dBm\n",
    "    #G_BD_dBi = Gain of the BD antenna in dBi\n",
    "    #G_AP_dBi = Gain of the AP antenna in dBi\n",
    "    #f = frequency of transmitted signal in Hertz\n",
    "    #d0 = reference distance of receiver from the transmitter in meters\n",
    "    #d = array of distances at which the loss needs to be calculated\n",
    "    #L = Other System Losses, for no Loss case L=1\n",
    "    #sigma = Standard deviation of log Normal distribution (in dB)\n",
    "    #n = path loss exponent\n",
    "    #Pr_dBm = Received power in dBm\n",
    "    #PL = path loss due to log normal shadowing\n",
    "    lamda_ = (3*10**8)/f #Wavelength in meters\n",
    "    K = 20*math.log10((4*np.pi*d0)/lamda_) # Path Loss factor *******- 10*n*math.log10(d0) - 10*math.log10(L) #path-loss factor\n",
    "    X = sigma*np.random.randn() #normal random variable\n",
    "    PL_dB = Gt_dBi + Gr_dBi + K + 10*n*math.log10(d/d0) + X  #PL(d) including antennas gains\n",
    "    \n",
    "    #Pr_dBm = Pt_dBm + PL #Receieved power in dBm at d meters\n",
    "    return PL_dB\n",
    "\n",
    "\n",
    "#AP-BD Channel Gain\n",
    "def APBD_ChannelGain(f):\n",
    "    Pt_dBm = 30 #1W AP transmission power 1W = 30dBm\n",
    "    Gt_dBi = 1 #Gain of the AP antenna in dBi \n",
    "    Gr_dBi = 1 #Gain of the BD antenna in dBi\n",
    "    d0 = 1 #reference distance of receiver from the transmitter in meters\n",
    "    d = 3 #distance between AP and BD #Throughput reduces significantly after 4 meters ()\n",
    "    L = 1 #Other System Losses, for no Loss case L=1\n",
    "    sigma=2 #Standard deviation of log Normal distribution (in dB)\n",
    "    n=2 # path loss exponent\n",
    "    G_dB = - logNormalShadowing(Pt_dBm,Gt_dBi,Gr_dBi,f,d0,d,L,sigma,n)\n",
    "    G = 10**(G_dB/10)\n",
    "    return G\n",
    "\n",
    "#J-AP Channel Gain\n",
    "def JAP_ChannelGain(f):\n",
    "    M = 5\n",
    "    P_J = 10 #np.random.randint(low=1,high=M,dtype = int) #choose a number between 1-4\n",
    "    Pt_dBm = 10*math.log10(P_J*1000) #selecting random jamming power between 1W - 4W\n",
    "    Gt_dBi = 1 #Gain of the J antenna in dBi \n",
    "    Gr_dBi = 1 #Gain of the AP antenna in dBi\n",
    "    d0 = 1 #reference distance of receiver from the transmitter in meters\n",
    "    d = 2 #distance between J and AP\n",
    "    L = 1 #Other System Losses, for no Loss case L=1\n",
    "    sigma=2 #Standard deviation of log Normal distribution (in dB)\n",
    "    n=2 # path loss exponent\n",
    "    G_dB = - logNormalShadowing(Pt_dBm,Gt_dBi,Gr_dBi,f,d0,d,L,sigma,n)\n",
    "    G = 10**(G_dB/10)\n",
    "    return G\n",
    "\n",
    "#J-BD Channel Gain\n",
    "def JBD_ChannelGain(f):\n",
    "    M = 5\n",
    "    P_J = 10 #np.random.randint(low=1,high=M,dtype = int) #choose a number between 1-4\n",
    "    Pt_dBm = 10*math.log10(P_J*1000) #selecting random jamming power between 1W - 4W\n",
    "    Gt_dBi = 1 #Gain of the J antenna in dBi \n",
    "    Gr_dBi = 1 #Gain of the AP antenna in dBi\n",
    "    d0 = 1 #reference distance of receiver from the transmitter in meters\n",
    "    d = 2 #distance between J and AP\n",
    "    L = 1 #Other System Losses, for no Loss case L=1\n",
    "    sigma=2 #Standard deviation of log Normal distribution (in dB)\n",
    "    n=2 # path loss exponent\n",
    "    G_dB = - logNormalShadowing(Pt_dBm,Gt_dBi,Gr_dBi,f,d0,d,L,sigma,n)\n",
    "    G = 10**(G_dB/10)\n",
    "    return G\n",
    "\n",
    "#Loopback Channel Gain (SIC)\n",
    "def loopback_Channel():\n",
    "    epsilon = 1e-10 #very small number\n",
    "    Pt_dBm = 30 #should be same as AP transmission power\n",
    "    #g_hat: feedback channel coefficient\n",
    "    #g_bar = 7e-5 #g_bar: feedback channel coefficient estimation\n",
    "    #g_tilde = 6e-5#g_tilde: estimation error\n",
    "    #g_hat = g_bar + (sqrt(epsilon))*g_tilde \n",
    "    G = 1 #E[g_tilde**2]=1\n",
    "    sigma = np.random.normal(loc=0.0, scale=1.0, size=None)\n",
    "    phi = -20 #dBm 30dBm reduction = 60dB reduction, by current techniques, it has been reported that SI can be canceled up to −81 dB in analog domain\n",
    "    combined_power_dB = (Pt_dBm+phi) + sigma**2\n",
    "    combined_power = 10**(combined_power_dB/10)\n",
    "    P = combined_power*epsilon*G\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a6f2d",
   "metadata": {},
   "source": [
    "### System Parameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce0fb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#System parameters\n",
    "N=10000 # Number of symbols to transmit\n",
    "L=16 # oversampling factor,L=Tb/Ts(Tb=bit period,Ts=sampling period) Tb (bit period) = symbol duration = (1/900MHz) = 1.1ns\n",
    "deltaf = 10e6 #frequency step\n",
    "Fc = 900e6 #900MHz #base carrier frequency\n",
    "Fs=L*Fc # sampling frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a446f4",
   "metadata": {},
   "source": [
    "#### AP and Jammer Signal ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19b69879",
   "metadata": {},
   "outputs": [],
   "source": [
    "### numpy.random.randn => Return a sample (or samples) from the “standard normal” distribution. \n",
    "#[Recall: The standard normal distribution is a specific type of normal distribution where the mean is equal to 0 \n",
    "#and the standard deviation is equal to 1.]\n",
    "### numpy.random.normal => Draw random samples from a normal (Gaussian) distribution. \n",
    "\n",
    "x = (np.random.randn(N*L) + 1j*np.random.randn(N*L))/np.sqrt(2) #Complex Gaussian with unity power of size N*L (N: # of symbols generated)\n",
    "x_J = (np.random.randn(N*L) + 1j*np.random.randn(N*L))/np.sqrt(2) #Complex Gaussian with unity power of size N*L (N: # of symbols generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d1ef0",
   "metadata": {},
   "source": [
    "### Noise ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a80ad94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.normal(loc=0.0, scale=1.0, size=None); loc => mean, scale => Standard deviation of the distribution (must be non-negative\n",
    "def complex_noise(mean, variance, size):\n",
    "    # Generate real and imaginary parts separately with normal distribution\n",
    "    real_part = np.random.normal(mean, np.sqrt(variance), size)\n",
    "    imag_part = np.random.normal(mean, np.sqrt(variance), size)\n",
    "    \n",
    "    # Combine real and imaginary parts into complex numbers\n",
    "    complex_noise = real_part + 1j * imag_part\n",
    "    \n",
    "    return complex_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ea6f4",
   "metadata": {},
   "source": [
    "### Jammer Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "91480c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jammer(n,K_ch):\n",
    "#     M = 5 #Jammer power level upper limit\n",
    "    P_J = 10 # np.random.randint(low=0,high=M,dtype = int) #choose a power level between 0 to 4 W and\n",
    "    #P_J = 10*math.log10(P_J_Watt) + 30  #convert P_J to dBm\n",
    "    deltaf = 10e6 #frequency step\n",
    "    Fc = 900e6 #900MHz #base carrier frequency\n",
    "    seq = np.arange(0,K_ch,dtype=int)\n",
    "    n = n+1\n",
    "    if n == len(seq):\n",
    "        n = 0\n",
    "    k_J = seq[n]\n",
    "    f_J = Fc + (k_J*deltaf)  #Jammer follows round robin\n",
    "    return (P_J,f_J,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d652ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean Jamming Power Received at AP ##\n",
    "def Mean_Jam_Power(P_J,f_J):\n",
    "    Gt_dBi = 1 #Gain of the J antenna in dBi \n",
    "    Gr_dBi = 1 #Gain of the AP antenna in dBi\n",
    "    lambda_ = (3*10**8)/(f_J)\n",
    "    d = 2 #distance between J and AP\n",
    "    n=2 # path loss exponent\n",
    "    PR_J = P_J*((Gt_dBi*Gr_dBi*(lambda_**2))/((d**n)*(4*np.pi)**2)) # in Watt\n",
    "    return PR_J "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c965f7",
   "metadata": {},
   "source": [
    "### State Space, Action Space ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3d5dbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BD - learning agent:\n",
    "#State: {E_B}: energy/battery level, we can discretize it. {1, 2, ..., L}\n",
    "#Action: harvesting or backscatter, e.g., {harvesting: 0; backscatter: 1}\n",
    "#Reward: {backscattered: reward 1; otherwise: reward 0}; This will not reflect the true throughput since the backscatter could be jammed.\n",
    "#Then BD can adopt Q-learning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "M = 10 #set number of jammer power level i.e. Jammer shuffles between M power levels\n",
    "\n",
    "def battery_state(mu):\n",
    "    \n",
    "    P_T = 1 #AP transmission power: 1W\n",
    "    #M = 10 #Jammer power level upper limit\n",
    "    P_J = np.random.randint(low=0,high=M,dtype = int) #choose a number between 0 to M-1\n",
    "    τ = 1#e-3 #duration of 1 time slot = 1ms\n",
    "    ξ = 0.8 #energy harvesting efficiency\n",
    "    #levels = 5 #5 discrete levels of BD energy\n",
    "    #E_levels = np.arange(1, levels+1, 1, dtype=int)\n",
    "    U_E = ξ*(P_J+P_T)*τ#*E_levels #Unit Energy #Signal energy doesn't depend upon frequency\n",
    "    E_BC = 0.75*U_E # Let 1 Unit energy used, fixed, by tag circuit during BC operation\n",
    "    E_EH = 0.25*U_E # Let 1 Unit energy used, fixed, by tag circuit during EH operation ### E_BC > E_EH\n",
    "    E_h = U_E #Amount of energy harvested #Energy harvested is function of time and power\n",
    "    E_B = mu*E_BC - (1 - mu)*E_EH + (1 - mu)*E_h + 1*U_E\n",
    "    return E_B\n",
    "\n",
    "state_space = [] #number of jammer power level = number of battery energy levels\n",
    "action_space = np.array([0,1])\n",
    "#print(P_J,E_B)\n",
    "#state_space_list = state_space.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7310e589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((M, len(action_space)))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f578c2c",
   "metadata": {},
   "source": [
    "### Trainning Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7bcf5",
   "metadata": {},
   "source": [
    "### Number of steps/time slots ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03652b5",
   "metadata": {},
   "source": [
    "- Steps = 200; E = 600; decay = 0.1; lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2b2f4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of steps each episode will have. 1 step = 1 time slot\n",
    "nSteps = 2 #number of time steps, in each time step, AP chooses a new channel ## 1 time step = 1 time slot\n",
    "\n",
    "#training parameters\n",
    "#In this case, number of steps per episode = number of time slots per episode\n",
    "#No terminating condition - each episode run up to \"nSteps\" - time slots\n",
    "num_episodes = 1 #total no. of episodes the agent will play during training\n",
    "max_steps_per_episode = nSteps #one of the terminating condition, max no. steps in a single episode\n",
    "\n",
    "learning_rate = 0.1 #high LR focus more on new, less on old; low LR learn nothing, use prior knowledge\n",
    "discount_rate = 0.99 #high DR focus more on distant reward, low DR focus more on immediate reward\n",
    "\n",
    "#epsilon-greedy\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b548d356",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EB 5.6000000000000005\n",
      "State Space [12.600000000000001, 5.6000000000000005]\n",
      "New State 1\n",
      "EB 1.4000000000000001\n",
      "State Space [12.600000000000001, 5.6000000000000005, 1.4000000000000001]\n",
      "New State 2\n"
     ]
    }
   ],
   "source": [
    "all_epochs = []\n",
    "all_iterations = []\n",
    "all_exploration_rate = []\n",
    "rewards_all_steps = []\n",
    "avg_reward_all_episodes = []\n",
    "avg_throughput_all_episodes = []\n",
    "rewards_current_step = 0\n",
    "iterations = 0\n",
    "epochs = 0\n",
    "reward = np.zeros(max_steps_per_episode)\n",
    "throughput = np.zeros(max_steps_per_episode)\n",
    "P_R_AP = np.zeros(max_steps_per_episode)\n",
    "P_N = np.zeros(max_steps_per_episode)\n",
    "SNR = np.zeros(max_steps_per_episode)\n",
    "EB = 0\n",
    "n=-1\n",
    "K_ch = 10\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    current_state = 0\n",
    "    epochs += 1\n",
    "    #rewards_current_step = 0\n",
    "    rewards_each_episode = []\n",
    "    throughput_each_episode = []\n",
    "    #avg_reward = 0\n",
    "    for step in range(max_steps_per_episode):\n",
    "        iterations += 1\n",
    "        # Exploration-exploitation trade-off\n",
    "        p0 = random.uniform(0, 1)\n",
    "        if p0 > exploration_rate:\n",
    "            action = np.argmax(q_table[current_state][:]) \n",
    "            #print('Exploited Action: ',action)\n",
    "        else:\n",
    "            action = random.choice(action_space) #randomly choose between 0 and 1\n",
    "            #print('Random Action: ',action)\n",
    "        mu = action #action decides whether backscatter (action = 1) or harvest (action =0)\n",
    "        \n",
    "        if (EB>=14):\n",
    "            mu = 1\n",
    "        elif (EB<=1):\n",
    "            mu = 0\n",
    "\n",
    "        \n",
    "        k =  np.random.randint(low=0,high=K_ch,dtype = int) #channel is decided randomly\n",
    "        f_k = Fc + (k*deltaf)\n",
    "        P_T = 1 #AP transmission power: 1W\n",
    "        \n",
    "        #Jammer Strategy:\n",
    "        (P_J,f_J,n) = Jammer(n,K_ch)\n",
    "#         PR_J = Mean_Jam_Power(P_J,f_J)\n",
    "#         BD_G = (PR_J*JBD_ChannelGain(f_J)**2 + P_T*APBD_ChannelGain(f_k)**2)\n",
    "\n",
    "#         print('AP Channel: ',f_k)\n",
    "#         print('Jammer Channel: ',f_J)\n",
    "        \n",
    "        P_R = ((mu**2)*P_T*(APBD_ChannelGain(f_k)**2)) #+ ((s_bb**2)*P_J*APBD_ChannelGain(f_k)*JBD_ChannelGain(f_J)) + P_J*JAP_ChannelGain(f_J) + loopback_Channel() + abs(complex_noise(0, 1, N*L))\n",
    "\n",
    "        P_R_AP[step] = sum(P_R)/(N*L);\n",
    "        \n",
    "#         Target_SNR = 15; #to calculate noise floor \n",
    "        Target_SNR = 20 # fixed noise floor, SNR = 20dB, #desired_SNR(k) #to get different noise floor for different channel\n",
    "        P_R_AP_tot = sum(P_R)/(N*L);\n",
    "        N_variance = P_R_AP_tot/10**(Target_SNR/10);\n",
    "        \n",
    "\n",
    "        unwanted_signal = 1*(f_J==f_k)*Mean_Jam_Power(P_J,f_J) + loopback_Channel() + N_variance*abs(complex_noise(0, 1, N*L))\n",
    "\n",
    "\n",
    "        P_N[step] = sum(unwanted_signal)/(N*L)\n",
    "\n",
    "        SNR[step] =  P_R_AP[step]/P_N[step]\n",
    "\n",
    "\n",
    "\n",
    "        #print(SNR)\n",
    "        throughput[step] = math.log2(1+SNR[step])\n",
    "        \n",
    "        #next state\n",
    "        EB = battery_state(mu) #calculate the EB value for this mu value at this time slot\n",
    "        print('EB',EB)       \n",
    "        reward[step] = mu #reward = 1 when backscatters (i.e. action =1), reward = 0 when harvests (i.e. action =0)\n",
    "        #print(reward[step])\n",
    "        state_space.append(EB)\n",
    "        print('State Space',state_space)\n",
    "        state_space_sorted = list(dict.fromkeys(state_space)) #avoiding repetition of same EB values\n",
    "        #print(state_space_sorted)\n",
    "        new_state = state_space_sorted.index(EB) #retrieve the index number that matches with the EB values \n",
    "        print('New State',new_state)\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[current_state][action] = (1 - learning_rate) * q_table[current_state][action] + \\\n",
    "        learning_rate * (reward[step] + discount_rate * np.max(q_table[new_state][:]))\n",
    "        \n",
    "        # Transition to the next state\n",
    "        current_state = new_state\n",
    "        # Add new reward        \n",
    "        rewards_current_step = reward[step]\n",
    "        #print(rewards_current_step)\n",
    "        rewards_each_episode.append(rewards_current_step)\n",
    "        rewards_all_steps.append(rewards_current_step)\n",
    "        \n",
    "        throughput_current_step = throughput[step]\n",
    "        #print(rewards_current_step)\n",
    "        throughput_each_episode.append(throughput_current_step)\n",
    "        \n",
    "        all_iterations.append(iterations)\n",
    "    \n",
    "    avg_reward = sum(rewards_each_episode)/max_steps_per_episode\n",
    "    avg_reward_all_episodes.append(avg_reward)\n",
    "    \n",
    "    avg_throughput = sum(throughput_each_episode)/max_steps_per_episode\n",
    "    avg_throughput_all_episodes.append(avg_throughput)\n",
    "    # Exploration rate decay (at the end of one episode, we need to update the exploration rate)\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    all_epochs.append(epochs)\n",
    "    all_exploration_rate.append(exploration_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5580c6d",
   "metadata": {},
   "source": [
    "### Throughput for different threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3059c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4fb8253",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_throughput_all_episodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate the moving average of real rewards\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m smoothed_rewards \u001b[38;5;241m=\u001b[39m moving_average(\u001b[43mavg_throughput_all_episodes\u001b[49m, window_size)\n\u001b[0;32m     12\u001b[0m fig1, ax1 \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,ncols \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m ax1\u001b[38;5;241m.\u001b[39msemilogy(all_epochs,avg_throughput_all_episodes,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-*\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearning Rate = 0.1\u001b[39m\u001b[38;5;124m'\u001b[39m, color \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_throughput_all_episodes' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to calculate a simple moving average\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Set the window size for smoothing\n",
    "window_size = 5\n",
    "\n",
    "# Calculate the moving average of real rewards\n",
    "smoothed_rewards = moving_average(avg_throughput_all_episodes, window_size)\n",
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots(nrows=1,ncols = 1)\n",
    "ax1.semilogy(all_epochs,avg_throughput_all_episodes,'-*', alpha=0.5, label = 'Learning Rate = 0.1', color ='blue')\n",
    "ax1.set_xlabel(r'Episode')\n",
    "ax1.set_ylabel(r'Average Reward')\n",
    "ax1.set_title('Average Reward over Multiple Episodes')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# Plot the smoothed rewards\n",
    "plt.plot(all_epochs[:-window_size+1], smoothed_rewards, label=f'Smoothed Rewards (Window Size={window_size})', color='blue')\n",
    "# ax1.legend(['Learning Rate = 0.1', 'Learning Rate = 0.5', 'Learning Rate = 0.9'])\n",
    "\n",
    "plt.tight_layout()\n",
    "save_results_to = 'C:/Users/Zunayeed/Desktop/PhD/WICL/NSF-AoF Project/Simulation BC EH/New Plots/Throughput/'\n",
    "plt.savefig(save_results_to + 'Avg_Reward_400S_600E.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c11145",
   "metadata": {},
   "source": [
    "- Steps = 200; E = 600; decay = 0.1; lr = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "538e3b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of steps each episode will have. 1 step = 1 time slot\n",
    "nSteps = 200 #number of time steps, in each time step, AP chooses a new channel ## 1 time step = 1 time slot\n",
    "\n",
    "#training parameters\n",
    "#In this case, number of steps per episode = number of time slots per episode\n",
    "#No terminating condition - each episode run up to \"nSteps\" - time slots\n",
    "num_episodes = 600 #total no. of episodes the agent will play during training\n",
    "max_steps_per_episode = nSteps #one of the terminating condition, max no. steps in a single episode\n",
    "\n",
    "learning_rate = 0.5 #high LR focus more on new, less on old; low LR learn nothing, use prior knowledge\n",
    "discount_rate = 0.99 #high DR focus more on distant reward, low DR focus more on immediate reward\n",
    "\n",
    "#epsilon-greedy\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487fd5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epochs = []\n",
    "all_iterations = []\n",
    "all_exploration_rate = []\n",
    "rewards_all_steps = []\n",
    "avg_reward_all_episodes2 = []\n",
    "rewards_current_step = 0\n",
    "iterations = 0\n",
    "epochs = 0\n",
    "reward = np.zeros(max_steps_per_episode)\n",
    "P_R_AP = np.zeros(max_steps_per_episode)\n",
    "P_N = np.zeros(max_steps_per_episode)\n",
    "SNR = np.zeros(max_steps_per_episode)\n",
    "EB = 0\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    current_state = 0\n",
    "    epochs += 1\n",
    "    #rewards_current_step = 0\n",
    "    rewards_each_episode2 = []\n",
    "    #avg_reward = 0\n",
    "    for step in range(max_steps_per_episode):\n",
    "        iterations += 1\n",
    "        # Exploration-exploitation trade-off\n",
    "        p0 = random.uniform(0, 1)\n",
    "        if p0 > exploration_rate:\n",
    "            action = np.argmax(q_table[current_state][:]) \n",
    "            #print('Exploited Action: ',action)\n",
    "        else:\n",
    "            action = random.choice(action_space) #randomly choose between 0 and 1\n",
    "            #print('Random Action: ',action)\n",
    "        mu = action #action decides whether backscatter (action = 1) or harvest (action =0)\n",
    "        \n",
    "        if (EB>=14):\n",
    "            mu = 1\n",
    "        elif (EB<=1):\n",
    "            mu = 0\n",
    "        #(s_bb,t)= bpsk_mod(mu,L) # BPSK modulation(waveform) - baseband\n",
    "        #k = 0 #fixed channel \n",
    "        #f_k = Fc + (k*deltaf)\n",
    "        #reflection coefficient: s_bb = -1 for mu = 0=> non-reflecting state, s_bb = 1 for mu = 1=>reflecting state\n",
    "        #unwanted_signal = (P_J*JAP_ChannelGain(f_k)*abs(x_J)) + loopback_Channel()*abs(x) + abs(noise((x+x_J),EbN0dB[step],L))\n",
    "        #y_AP = ((s_bb[step]**2)*P_T*APBD_ChannelGain(f_k)*abs(x)) + ((s_bb[step]**2)*P_J*APBD_ChannelGain(f_k)*abs(x)*JBD_ChannelGain(f_k)*abs(x_J)) + \\\n",
    "        #unwanted_signal\n",
    "        #P_R_AP[step] = sum(y_AP)/N\n",
    "        #P_N[step] = sum(unwanted_signal)/N\n",
    "        #SNR[step] =  P_R_AP[step]/P_N[step]\n",
    "        #print('AP uses channel '+str(k)+' having frequency '+str(f_k))\n",
    "        \n",
    "        #next state\n",
    "        EB = battery_state(mu) #calculate the EB value for this mu value at this time slot\n",
    "               \n",
    "        reward[step] = mu #reward = 1 when backscatters (i.e. action =1), reward = 0 when harvests (i.e. action =0)\n",
    "        #print(reward[step])\n",
    "        state_space.append(EB)\n",
    "        state_space_sorted = list(dict.fromkeys(state_space)) #avoiding repetition of same EB values\n",
    "        #print(state_space_sorted)\n",
    "        new_state = state_space_sorted.index(EB) #retrieve the index number that matches with the EB values \n",
    "        #print(new_state)\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[current_state][action] = (1 - learning_rate) * q_table[current_state][action] + \\\n",
    "        learning_rate * (reward[step] + discount_rate * np.max(q_table[new_state][:]))\n",
    "        \n",
    "        # Transition to the next state\n",
    "        current_state = new_state\n",
    "        # Add new reward        \n",
    "        rewards_current_step = reward[step]\n",
    "        #print(rewards_current_step)\n",
    "        rewards_each_episode2.append(rewards_current_step)\n",
    "        rewards_all_steps.append(rewards_current_step)\n",
    "        \n",
    "        all_iterations.append(iterations)\n",
    "    avg_reward2 = sum(rewards_each_episode2)/max_steps_per_episode\n",
    "    avg_reward_all_episodes2.append(avg_reward2)\n",
    "    # Exploration rate decay (at the end of one episode, we need to update the exploration rate)\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    all_epochs.append(epochs)\n",
    "    all_exploration_rate.append(exploration_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776c03d",
   "metadata": {},
   "source": [
    "- Steps = 200; E = 600; decay = 0.1; lr = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9fe9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of steps each episode will have. 1 step = 1 time slot\n",
    "nSteps = 200 #number of time steps, in each time step, AP chooses a new channel ## 1 time step = 1 time slot\n",
    "\n",
    "#training parameters\n",
    "#In this case, number of steps per episode = number of time slots per episode\n",
    "#No terminating condition - each episode run up to \"nSteps\" - time slots\n",
    "num_episodes = 600 #total no. of episodes the agent will play during training\n",
    "max_steps_per_episode = nSteps #one of the terminating condition, max no. steps in a single episode\n",
    "\n",
    "learning_rate = 0.9 #high LR focus more on new, less on old; low LR learn nothing, use prior knowledge\n",
    "discount_rate = 0.99 #high DR focus more on distant reward, low DR focus more on immediate reward\n",
    "\n",
    "#epsilon-greedy\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97ac53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epochs = []\n",
    "all_iterations = []\n",
    "all_exploration_rate = []\n",
    "rewards_all_steps = []\n",
    "avg_reward_all_episodes3 = []\n",
    "rewards_current_step = 0\n",
    "iterations = 0\n",
    "epochs = 0\n",
    "reward = np.zeros(max_steps_per_episode)\n",
    "P_R_AP = np.zeros(max_steps_per_episode)\n",
    "P_N = np.zeros(max_steps_per_episode)\n",
    "SNR = np.zeros(max_steps_per_episode)\n",
    "EB = 0\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    current_state = 0\n",
    "    epochs += 1\n",
    "    #rewards_current_step = 0\n",
    "    rewards_each_episode3 = []\n",
    "    #avg_reward = 0\n",
    "    for step in range(max_steps_per_episode):\n",
    "        iterations += 1\n",
    "        # Exploration-exploitation trade-off\n",
    "        p0 = random.uniform(0, 1)\n",
    "        if p0 > exploration_rate:\n",
    "            action = np.argmax(q_table[current_state][:]) \n",
    "            #print('Exploited Action: ',action)\n",
    "        else:\n",
    "            action = random.choice(action_space) #randomly choose between 0 and 1\n",
    "            #print('Random Action: ',action)\n",
    "        mu = action #action decides whether backscatter (action = 1) or harvest (action =0)\n",
    "        \n",
    "        if (EB>=14):\n",
    "            mu = 1\n",
    "        elif (EB<=1):\n",
    "            mu = 0\n",
    "        #(s_bb,t)= bpsk_mod(mu,L) # BPSK modulation(waveform) - baseband\n",
    "        #k = 0 #fixed channel \n",
    "        #f_k = Fc + (k*deltaf)\n",
    "        #reflection coefficient: s_bb = -1 for mu = 0=> non-reflecting state, s_bb = 1 for mu = 1=>reflecting state\n",
    "        #unwanted_signal = (P_J*JAP_ChannelGain(f_k)*abs(x_J)) + loopback_Channel()*abs(x) + abs(noise((x+x_J),EbN0dB[step],L))\n",
    "        #y_AP = ((s_bb[step]**2)*P_T*APBD_ChannelGain(f_k)*abs(x)) + ((s_bb[step]**2)*P_J*APBD_ChannelGain(f_k)*abs(x)*JBD_ChannelGain(f_k)*abs(x_J)) + \\\n",
    "        #unwanted_signal\n",
    "        #P_R_AP[step] = sum(y_AP)/N\n",
    "        #P_N[step] = sum(unwanted_signal)/N\n",
    "        #SNR[step] =  P_R_AP[step]/P_N[step]\n",
    "        #print('AP uses channel '+str(k)+' having frequency '+str(f_k))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #next state\n",
    "        EB = battery_state(mu) #calculate the EB value for this mu value at this time slot\n",
    "               \n",
    "        reward[step] = mu #reward = 1 when backscatters (i.e. action =1), reward = 0 when harvests (i.e. action =0)\n",
    "        #print(reward[step])\n",
    "        state_space.append(EB)\n",
    "        state_space_sorted = list(dict.fromkeys(state_space)) #avoiding repetition of same EB values\n",
    "        #print(state_space_sorted)\n",
    "        new_state = state_space_sorted.index(EB) #retrieve the index number that matches with the EB values \n",
    "        #print(new_state)\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[current_state][action] = (1 - learning_rate) * q_table[current_state][action] + \\\n",
    "        learning_rate * (reward[step] + discount_rate * np.max(q_table[new_state][:]))\n",
    "        \n",
    "        # Transition to the next state\n",
    "        current_state = new_state\n",
    "        # Add new reward        \n",
    "        rewards_current_step = reward[step]\n",
    "        #print(rewards_current_step)\n",
    "        rewards_each_episode3.append(rewards_current_step)\n",
    "        rewards_all_steps.append(rewards_current_step)\n",
    "        \n",
    "        all_iterations.append(iterations)\n",
    "    avg_reward3 = sum(rewards_each_episode3)/max_steps_per_episode\n",
    "    avg_reward_all_episodes3.append(avg_reward3)\n",
    "    # Exploration rate decay (at the end of one episode, we need to update the exploration rate)\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    all_epochs.append(epochs)\n",
    "    all_exploration_rate.append(exploration_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcc5df",
   "metadata": {},
   "source": [
    "## Plots ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7df04fa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (0,) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Plot the original real rewards with reduced opacity\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# plt.plot(episodes, real_rewards, label='Real Rewards', color='blue', alpha=0.5)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Plot the smoothed rewards\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_epochs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothed_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSmoothed Rewards (Window Size=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mwindow_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(all_epochs[:\u001b[38;5;241m-\u001b[39mwindow_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], smoothed_rewards2, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmoothed Rewards (Window Size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(all_epochs[:\u001b[38;5;241m-\u001b[39mwindow_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], smoothed_rewards3, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmoothed Rewards (Window Size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:2757\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2755\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   2758\u001b[0m         \u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39mscalex, scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   2759\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1631\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1632\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    311\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 312\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py:498\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (0,) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAd50lEQVR4nO3de5gdVZnv8e8vCchVQBNQciEIIRodQOSmgraAEFDEC2MISATBmCMo4xkV9DiCRx111CN6QENwIqLIZRAwOgjoQAc4gOIlRBIMxoAkhovhYkjwCEne+WOtdle23dXVna7eO92/z/Psp+uydtVba++ut2pV1dqKCMzMzHoyotUBmJlZe3OiMDOzUk4UZmZWyonCzMxKOVGYmVkpJwozMyvlRGHDjqSQtGer46iDpA5JK0rmT5C0RtLICsuamOtq1MBG2et6T5J00wAvsyXbMlQ4UbSYpE5JT0p6Xqtj2VSSzpP0XN4RPSXpDkmvbnVcm4PCjuxXTdNHS3pW0oP9XO6Dko7oGo+IhyJiu4hYv4kh97be4neh6/VUlfdGxGURcWSd8VnfOFG0kKSJwKFAAG+pYfmtOHq6MiK2A0YDtwD/0YIYgJZtf696iWtbSa8ojJ8IPFBzSHW5MielrteOrQ7I+seJorVmAHcBlwDvBpD0vHw0/redhaQxkv4iaec8/mZJCwpH7XsXyj4o6WxJC4G1kkZJOkfS7yU9LWmxpLcVyo+U9GVJqyQ9IOnM4im6pB0k/bukhyX9UdJnqjRbRMQ64DJgrKQxvS1L0h8kvSoPvyvHMCWPny7pujx8oKQ787Y/LOkCSVsWticknSHpd8Dv8rSP5LIrJb2nLG5Ju0qaJ+kJSUslvbcw/S+SXlAo+8pcb1vk8fdIui+fId4oabeyuHrwHfJ3IZsBXNoU40ZNZ5IukfSZbrblO8AE4If5iP6jzU0w+Yz2c5J+LunPkn5Q3Mam5fXru9DDskLSByUty3X4RUkj8rxTJN2ehyXpK5Iey/Et7PrfyPFcKulP+fvzicIyRkr6Ul72MuBNVbdF0p6S5uf1rZJ0ZX+2cShxomitGaSd6WXAUZJ2iYi/AtcA0wvl3gnMj4jHJO0HzAXeB7wQuAiYp42brqaT/jF2zDvs35POXHYAPgV8V9KLc9n3AkcD+wL7AW9tivHbwDpgT+CVwJHA6b1tWN55zwAeB56ssKz5QEcefh2wDHh9YXx+Hl4PfIh0xvJq4HDg/U2rfytwEDBF0lTgw8AbgUnAEZS7HFgB7AocD/yrpMMjYiVwJ/COQtkTgasj4jlJbwU+DrwdGAPclpfVbVwl6/8ucELe0b0M2B74WS8xdysiTgYeAo7NR/T/1kPRGcB7SNu8DvhaD+X69V0o8TZgf9L37rgcQ7MjSZ//XsCOwDTSdwrg/5K+0y8hfVdmAKfmee8F3pzj3J/0WVbdlk8DNwE7AePyeoa3iPCrBS/gEOA5YHQe/y3woTx8BLCsUPb/ATPy8DeATzctawnw+jz8IPCeXta9ADguD98MvK8w7whSU9goYBfgr8DWhfnTgVt6WO55wLPAU6Qd+uNAR55XuizgNGBeHr6P9E97RR7/A7BfD+v8J+DawngAhxXG5wKfL4zvlcvs2c2yxue4ty9M+xxwSR4+Hbg5DwtYDrwuj/8YOK3wvhHAM8Bu3cXVzbonFur9p8BRwOeB/5U/kwebtnHPwvglwGfycAewojDvQeCI7taTxzub6mdK/gxHNsW0Kd+FrtctTdswtTD+fuC/8vApwO15+DDgfuBgYESh/Mgcz5TCtPcBnYXv9azCvCOrbgvpDG4OMK7OfcDm9PIZReu8G7gpIlbl8e/RaHK4Gdha0kG5+WJf4No8bzfgn3PTy1NKFwjHk44GuywvrkjSDDWaqp4CXkE6Iie/b3kP790N2AJ4uPDei4CdS7brqkht0bsA9wKvqris+cChkl5E2glcCbxW6TrODqTkhqS9JP1I0iOSVgP/WtiW7rahefv+UBL7rsATEfF0U/mxefhq4NWSdiUd5QbpzKFr+75a2LYnSMlkbGFZG30uJS4l7Synk84w6tZcP1vw93Xa7+9C4fWGXta7a9N8IuJm4ALgQuBRSXMkPT/HtyUbf57Fz6rsc+9tWz5K+ux+LmlRb82Vw0FbXuwb6iRtTWpOGinpkTz5ecCOkvaJiHskXUXaUTwK/Kiw81oOfDYiPluyir91CZwTzcWkJpo7I2K9pAWkfwSAh0mn113GF4aXk468RkdqwqosIlZJeh9wt6Tv9basiFgq6Rngg8CtEfF0rpuZpKPLDbnoN4BfA9NzmX/i75sVil0iP9y0TRNKwl4JvEDS9oX6ngD8Mcf4lNJtm+8EXgZcHvkQlMbnclnJ8qt21fx90s7xlxHxB0mTmuY/A2xTGH8Rqbmsv+tsrp/ngFUM0Hehl/UuKqx3ZXeFIuJrwNeUrtFdBXyEdMbyHGmnv7iwjD/m4bLPvbfv4iOkpiskHQL8VNKtEbG0j9s3ZPiMojXeSmrimEI6W9iXtOO5jdTOCukMYxpwUh7ucjEwK59tSNK2kt4kafse1rUtaWfxJwBJp5LOKLpcBZwlaaykHYGzu2ZExMOkttovS3q+pBGS9pD0eiqIiN8CNwIfrbis+cCZNK5HdDaNQ2qzXw2skfRS4H/0EsZVwCmSpkjaBji3JN7lwB3A5yRtpXSTwGmka0hdvkf6jN7Bxp/LbOBjkl4Of7tY+o+9xNZTHGtJTS49tf8vAE7M1zGm0riW051HSW34Zd5VqJ//TbrustHts5v6XejBRyTtJGk8cBbpLHIjkg7I3/UtgLXA/wfW5/iuAj4raft8QPQ/aZyBXQV8UNI4STsB51TdFkn/KKnr4OlJ0v9PrbcTtzsnitZ4N/CtSPe0P9L1Ih1FniRpVET8jPSPsSup/RuAiPgF6WjnAtKXeCmpmaJbEbEY+DLpQuyjwD+Qrnl0uZj0T7OQdKR+PekiX9c/xgzSKf7ivL6rgRdT3ReBmflosLdlzSclglt7GId0YfpE4Okce+kdKRHxY+B8UnPe0vy3zHRS2/xKUnPfuRHxk8L8eaSL4o9GxD2F9VwLfAG4IjeJ3Uu6SaBfIuIXEfH7HmafBRxLavc/CbiuZFGfAz6Rm1g+3EOZ75CuczwCbEU6q+tOX78L07TxcxRr8vegyw+AX5IS338C/97NMp5P+pyfJDUfPQ58Kc/7AOl/ZBlwOylxz83zLiYdpNwD/Ip0g0jVbTkA+JmkNaTP+6yI2FxvUR4Qapw5m4Gko4HZEbFbr4VtsyepE/huRHxzkNcbwKTh3JyzOfEZxTAnaWtJxyg9bzGW1DRzbW/vM7Pho7ZEIWmu0kMy9/YwX5K+pvRQ00Kl5wNs8In0bMWTpKan+4BPtjQiM2srtTU9SXodsAa4NCJe0c38Y0htjMeQHkL6akQcVEswZmbWb7WdUUTEraR7yXtyHCmJRETcRbo1tC8XSc3MbBC08jmKsWz8QMyKPO3h5oKSZpLup2errbZ61YQJZbfCDx8bNmxgxAhfZgLXRZHrosF10XD//feviogx/XlvKxOFupnWbTtYRMwhPVLP5MmTY8mSJXXGtdno7Oyko6Oj1WG0BddFg+uiwXXRIKmsV4JSrUy1K9j4yclx9PBkppmZtU4rE8U8YEa+++lg4M/5iUkzM2sjtTU9Sbqc1JPlaKWfZjyX1BEXETGb9ATwMaSnZZ+h0T2wmZm1kdoSRURM72V+AGfUtX4zMxsYvh3AzMxKOVGYmVkpJwozMyvlRGFmZqWcKMzMrJQThZmZlXKiMDOzUk4UZmZWyonCzMxKOVGYmVkpJwozMyvlRGFmZqWcKMzMrJQThZmZlXKiMDOzUk4UZmZWyonCzMxKOVGYmVkpJwozMyvlRGFmZqWcKMzMrJQThZmZlXKiMDOzUk4UZmZWyonCzMxKOVGYmVkpJwozMyvlRGFmZqWcKMzMrJQThZmZlXKiMDOzUk4UZmZWyonCzMxKOVGYmVkpJwozMytVa6KQNFXSEklLJZ3TzfwdJP1Q0j2SFkk6tc54zMys72pLFJJGAhcCRwNTgOmSpjQVOwNYHBH7AB3AlyVtWVdMZmbWd3WeURwILI2IZRHxLHAFcFxTmQC2lyRgO+AJYF2NMZmZWR+NqnHZY4HlhfEVwEFNZS4A5gErge2BaRGxoXlBkmYCMwHGjBlDZ2dnHfFudtasWeO6yFwXDa6LBtfFwKgzUaibadE0fhSwADgM2AP4iaTbImL1Rm+KmAPMAZg8eXJ0dHQMeLCbo87OTlwXieuiwXXR4LoYGHU2Pa0AxhfGx5HOHIpOBa6JZCnwAPDSGmMyM7M+qjNR3A1MkrR7vkB9AqmZqegh4HAASbsAk4FlNcZkZmZ9VFvTU0Ssk3QmcCMwEpgbEYskzcrzZwOfBi6R9BtSU9XZEbGqrpjMzKzv6rxGQURcD1zfNG12YXglcGSdMZiZ2abxk9lmZlbKicLMzEo5UZiZWSknCjMzK+VEYWZmpZwozMyslBOFmZmVcqIwM7NSThRmZlbKicLMzEo5UZiZWSknCjMzK+VEYWZmpZwozMyslBOFmZmVcqIwM7NSThRmZlbKicLMzEo5UZiZWSknCjMzK+VEYWZmpUb1NEPS28veGBHXDHw4ZmbWbnpMFMCx+e/OwGuAm/P4G4BOwInCzGwY6DFRRMSpAJJ+BEyJiIfz+IuBCwcnPDMza7Uq1ygmdiWJ7FFgr5riMTOzNlPW9NSlU9KNwOVAACcAt9QalZmZtY1eE0VEnCnpbcDr8qQ5EXFtvWGZmVm7KE0UkkYACyPiFYCTg5nZMFR6jSIiNgD3SJowSPGYmVmbqXKN4sXAIkk/B9Z2TYyIt9QWlZmZtY0qieJTtUdhZmZtq8rF7PmDEYiZmbWnXp+jkHSwpLslrZH0rKT1klYPRnBmZtZ6VR64uwCYDvwO2Bo4PU8zM7NhoFLvsRGxFBgZEesj4ltAR5X3SZoqaYmkpZLO6aFMh6QFkhZJcjOXmVmbqXIx+xlJWwILJP0b8DCwbW9vkjSS1CfUG4EVwN2S5kXE4kKZHYGvA1Mj4iFJO/djG8zMrEZVzihOzuXOJN0eOx54R4X3HQgsjYhlEfEscAVwXFOZE4FrIuIhgIh4rGrgZmY2OKqcUewB/CkiVtO3W2XHAssL4yuAg5rK7AVsIakT2B74akRc2rwgSTOBmQBjxoyhs7OzD2EMXWvWrHFdZK6LBtdFg+tiYFRJFKcAsyU9DtyWX7dHxJO9vE/dTItu1v8q4HDShfI7Jd0VEfdv9KaIOcAcgMmTJ0dHR0eFsIe+zs5OXBeJ66LBddHguhgYVZ6jmAEgaVfgeNJ1h10rvHcFqZmqyzhgZTdlVkXEWmCtpFuBfYD7MTOztlDlOYp3SboIuBo4gnRr7KEVln03MEnS7vli+AnAvKYyPwAOlTRK0jakpqn7+rIBZmZWrypNT+cDvwdmA7dExINVFhwR6ySdCdwIjATmRsQiSbPy/NkRcZ+kG4CFwAbgmxFxb983w8zM6lKl6Wm0pJeTfo/is5ImAUsi4uQK770euL5p2uym8S8CX+xT1GZmNmiqND09H5gA7AZMBHYgHf2bmdkwUKXp6fbC64KIWFFvSGZm1k6qND3tDSBp23x3kpmZDSNVmp5eLWkx+W4kSftI+nrtkZmZWVuo0oXH+cBRwOMAEXEP6cK2mZkNA1V7j13eNGl9DbGYmVkbqnIxe7mk1wCRH5z7IH4ozsxs2KhyRjELOIPUyd8KYF/g/TXGZGZmbaTKXU+rgJO6xiXtREoUn60xLjMzaxM9nlFIGi9pjqQfSTpN0jaSvgQsAfwDQ2Zmw0TZGcWlwHzg+8BU4C5gEbB3RDwyCLGZmVkbKEsUL4iI8/LwjZIeBQ6IiL/WH5aZmbWL0msU+XpE1w8QPQJsI2lbgIh4oubYzMysDZQlih2AX7LxL9X9Kv8N4CV1BWVmZu2jx0QRERMHMQ4zM2tTlZ7MNjOz4cuJwszMSjlRmJlZqUqJQtIhkk7Nw2Mk7V5vWGZm1i6q/B7FucDZwMfypC2A79YZlJmZtY8qZxRvA94CrAWIiJXA9nUGZWZm7aNKong2IoL07ARdD9yZmdnwUCVRXCXpImBHSe8FfgpcXG9YZmbWLqp0M/4lSW8EVgOTgU9GxE9qj8zMzNpClV+4IycGJwczs2Go10Qh6Wny9YmCPwO/AP45IpbVEZiZmbWHKmcU/wdYCXyP1EHgCcCLSD9gNBfoqCs4MzNrvSoXs6dGxEUR8XRErI6IOcAxEXElsFPN8ZmZWYtVSRQbJL1T0oj8emdhXnOTlJmZDTFVEsVJwMnAY8CjefhdkrYGzqwxNjMzawNVbo9dBhzbw+zbBzYcMzNrN1XuetoKOA14ObBV1/SIeE+NcZmZWZuo0vT0HdJdTkcB84FxwNN1BmVmZu2jSqLYMyL+BVgbEd8G3gT8Q71hmZlZu6iSKJ7Lf5+S9ApgB2BibRGZmVlbqZIo5kjaCfgEMA9YDHyhysIlTZW0RNJSSeeUlDtA0npJx1eK2szMBk3pxWxJI4DVEfEkcCvwkqoLljQSuBB4I7ACuFvSvIhY3E25LwA39jF2MzMbBKVnFBGxgf4/K3EgsDQilkXEs8AVwHHdlPsA8H3ScxpmZtZmqvT19BNJHwauJP/KHUBEPNHL+8YCywvjK4CDigUkjSX9gt5hwAE9LUjSTGAmwJgxY+js7KwQ9tC3Zs0a10XmumhwXTS4LgZGlUTR9bzEGYVpQe/NUOpmWnOXH+cDZ0fEeqm74vlNqX+pOQCTJ0+Ojo6OXlY9PHR2duK6SFwXDa6LBtfFwKjyZPbu/Vz2CmB8YXwcqRfaov2BK3KSGA0cI2ldRFzXz3WamdkA6/WuJ0nbSPqEpDl5fJKkN1dY9t3AJEm7S9qS1D35vGKBiNg9IiZGxETgauD9ThJmZu2lyu2x3wKeBV6Tx1cAn+ntTRGxjnQh/EbgPuCqiFgkaZakWf2M18zMBlmVaxR7RMQ0SdMBIuIvKrugUBAR1wPXN02b3UPZU6os08zMBleVM4pnc5fiASBpD+CvtUZlZmZto8oZxXnADcB4SZcBrwVOqTEmMzNrI1XuerpJ0i+Bg0m3vJ4VEatqj8zMzNpCld+jmAdcDsyLiLW9lTczs6GlyjWKLwOHAosl/Yek4/OPGZmZ2TBQpelpPjA/d953GPBeYC7w/JpjMzOzNlDlYjb5rqdjgWnAfsC36wzKzMzaR5VrFFeSOvO7gdRteGfuVdbMzIaBKmcU3wJOjIj1AJJeK+nEiDijl/eZmdkQUOUaxQ2S9s1PZk8DHgCuqT0yMzNrCz0mCkl7kTrymw48Tvo9CkXEGwYpNjMzawNlZxS/BW4Djo2IpQCSPjQoUZmZWdsoe47iHcAjwC2SLpZ0ON3/GJGZmQ1hPSaKiLg2IqYBLwU6gQ8Bu0j6hqQjByk+MzNrsV6fzI6ItRFxWUS8mfQrdQuAc+oOzMzM2kOVLjz+JiKeiIiLIuKwugIyM7P20qdEYWZmw48ThZmZlXKiMDOzUk4UZmZWyonCzMxKOVGYmVkpJwozMyvlRGFmZqWcKMzMrJQThZmZlXKiMDOzUk4UZmZWyonCzMxKOVGYmVkpJwozMyvlRGFmZqWcKMzMrJQThZmZlao1UUiaKmmJpKWS/u53tiWdJGlhft0haZ864zEzs76rLVFIGglcCBwNTAGmS5rSVOwB4PURsTfwaWBOXfGYmVn/1HlGcSCwNCKWRcSzwBXAccUCEXFHRDyZR+8CxtUYj5mZ9cOoGpc9FlheGF8BHFRS/jTgx93NkDQTmAkwZswYOjs7ByjEzduaNWtcF5nrosF10eC6GBh1Jgp1My26LSi9gZQoDulufkTMITdLTZ48OTo6OgYoxM1bZ2cnrovEddHgumhwXQyMOhPFCmB8YXwcsLK5kKS9gW8CR0fE4zXGY2Zm/VDnNYq7gUmSdpe0JXACMK9YQNIE4Brg5Ii4v8ZYzMysn2o7o4iIdZLOBG4ERgJzI2KRpFl5/mzgk8ALga9LAlgXEfvXFZOZmfVdnU1PRMT1wPVN02YXhk8HTq8zBjMz2zR+MtvMzEo5UZiZWSknCjMzK+VEYWZmpZwozMyslBOFmZmVcqIwM7NSThRmZlbKicLMzEo5UZiZWSknCjMzK+VEYWZmpZwozMyslBOFmZmVcqIwM7NSThRmZlbKicLMzEo5UZiZWSknCjMzK+VEYWZmpZwozMyslBOFmZmVcqIwM7NSThRmZlbKicLMzEo5UZiZWSknCjMzK+VEYWZmpZwozMyslBOFmZmVcqIwM7NSThRmZlbKicLMzEo5UZiZWSknCjMzK1VropA0VdISSUslndPNfEn6Wp6/UNJ+dcZjZmZ9V1uikDQSuBA4GpgCTJc0panY0cCk/JoJfKOueMzMrH/qPKM4EFgaEcsi4lngCuC4pjLHAZdGchewo6QX1xiTmZn10agalz0WWF4YXwEcVKHMWODhYiFJM0lnHAB/lXTvwIa62RoNrGp1EG3CddHgumhwXTRM7u8b60wU6mZa9KMMETEHmAMg6RcRsf+mh7f5c100uC4aXBcNrosGSb/o73vrbHpaAYwvjI8DVvajjJmZtVCdieJuYJKk3SVtCZwAzGsqMw+Yke9+Ohj4c0Q83LwgMzNrndqaniJinaQzgRuBkcDciFgkaVaePxu4HjgGWAo8A5xaYdFzagp5c+S6aHBdNLguGlwXDf2uC0X83SUBMzOzv/GT2WZmVsqJwszMSrVtonD3Hw0V6uKkXAcLJd0haZ9WxDkYequLQrkDJK2XdPxgxjeYqtSFpA5JCyQtkjR/sGMcLBX+R3aQ9ENJ9+S6qHI9dLMjaa6kx3p61qzf+82IaLsX6eL374GXAFsC9wBTmsocA/yY9CzGwcDPWh13C+viNcBOefjo4VwXhXI3k26WOL7Vcbfwe7EjsBiYkMd3bnXcLayLjwNfyMNjgCeALVsdew118TpgP+DeHub3a7/ZrmcU7v6jode6iIg7IuLJPHoX6XmUoajK9wLgA8D3gccGM7hBVqUuTgSuiYiHACJiqNZHlboIYHtJArYjJYp1gxtm/SLiVtK29aRf+812TRQ9de3R1zJDQV+38zTSEcNQ1GtdSBoLvA2YPYhxtUKV78VewE6SOiX9UtKMQYtucFWpiwuAl5Ee6P0NcFZEbBic8NpKv/abdXbhsSkGrPuPIaDydkp6AylRHFJrRK1TpS7OB86OiPXp4HHIqlIXo4BXAYcDWwN3SrorIu6vO7hBVqUujgIWAIcBewA/kXRbRKyuObZ206/9ZrsmCnf/0VBpOyXtDXwTODoiHh+k2AZblbrYH7giJ4nRwDGS1kXEdYMS4eCp+j+yKiLWAmsl3QrsAwy1RFGlLk4FPh+poX6ppAeAlwI/H5wQ20a/9pvt2vTk7j8aeq0LSROAa4CTh+DRYlGvdRERu0fExIiYCFwNvH8IJgmo9j/yA+BQSaMkbUPqvfm+QY5zMFSpi4dIZ1ZI2oXUk+qyQY2yPfRrv9mWZxRRX/cfm52KdfFJ4IXA1/OR9LoYgj1mVqyLYaFKXUTEfZJuABYCG4BvRsSQ66K/4vfi08Alkn5Dan45OyKGXPfjki4HOoDRklYA5wJbwKbtN92Fh5mZlWrXpiczM2sTThRmZlbKicLMzEo5UZiZWSknCjMzK+VEYcNe7mV2QeHVY6+0ufysgegOQ9KDkkZv6nLM6ubbY23Yk7QmIrZrwXofBPYfivfz29DiMwqzHuQj/i9I+nl+7Zmnnyfpw3n4g5IW5779r8jTXiDpujztrty9CpJeKOkmSb+WdBGFfnckvSuvY4GkiySNbMEmm3XLicIMtm5qeppWmLc6Ig4k9T56fjfvPQd4ZUTsDczK0z4F/DpP+zhwaZ5+LnB7RLyS1JXCBABJLwOmAa+NiH2B9cBJA7mBZpuiLbvwMBtkf8k76O5cXvj7lW7mLwQuk3QdcF2edgjwDoCIuDmfSexA+lGZt+fp/ymp6zdEDif18np37oJla4b2b2nYZsaJwqxc9DDc5U2kBPAW4F8kvZzyrpy7W4aAb0fExzYlULO6uOnJrNy0wt87izMkjQDGR8QtwEdJPz26HXAruelIUgepq+/VTdOPBnbKi/ov4HhJO+d5L5C0W21bZNZHPqMwy9coCuM3RETXLbLPk/Qz0kHV9Kb3jQS+m5uVBHwlIp6SdB7wLUkLST10vjuX/xRwuaRfAfNJXV8TEYslfQK4KSef54AzgD8M8Haa9YtvjzXrgW9fNUvc9GRmZqV8RmFmZqV8RmFmZqWcKMzMrJQThZmZlXKiMDOzUk4UZmZW6r8B7HV03uLSDxYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data (replace this with your own data)\n",
    "# episodes = np.arange(1, 101)  # episode numbers\n",
    "# real_rewards = np.random.normal(0, 1, size=(100,))  # replace with your actual rewards\n",
    "\n",
    "# Function to calculate a simple moving average\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Set the window size for smoothing\n",
    "window_size = 5\n",
    "\n",
    "# Calculate the moving average of real rewards\n",
    "smoothed_rewards = moving_average(avg_reward_all_episodes, window_size)\n",
    "smoothed_rewards2 = moving_average(avg_reward_all_episodes2, window_size)\n",
    "smoothed_rewards3 = moving_average(avg_reward_all_episodes3, window_size)\n",
    "\n",
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots(nrows=1,ncols = 1)\n",
    "# ax1.semilogy(all_epochs,avg_reward_all_episodes,'-*', alpha=0.5, label = 'Learning Rate = 0.1', color ='blue')\n",
    "# ax1.semilogy(all_epochs,avg_reward_all_episodes2,'-*', alpha=0.5, label = 'Learning Rate = 0.5', color ='green')\n",
    "# ax1.semilogy(all_epochs,avg_reward_all_episodes3,'-*', alpha=0.5, label = 'Learning Rate = 0.9', color ='red')\n",
    "ax1.set_xlabel(r'Episode')\n",
    "ax1.set_ylabel(r'Average Reward')\n",
    "ax1.set_title('Average Reward over Multiple Episodes')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# Plot the original real rewards with reduced opacity\n",
    "# plt.plot(episodes, real_rewards, label='Real Rewards', color='blue', alpha=0.5)\n",
    "\n",
    "# Plot the smoothed rewards\n",
    "plt.plot(all_epochs[:-window_size+1], smoothed_rewards, label=f'Smoothed Rewards (Window Size={window_size})', color='blue')\n",
    "plt.plot(all_epochs[:-window_size+1], smoothed_rewards2, label=f'Smoothed Rewards (Window Size={window_size})', color='green')\n",
    "plt.plot(all_epochs[:-window_size+1], smoothed_rewards3, label=f'Smoothed Rewards (Window Size={window_size})', color='red')\n",
    "ax1.legend(['Learning Rate = 0.1', 'Learning Rate = 0.5', 'Learning Rate = 0.9'])\n",
    "# Add labels and legend\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Reward')\n",
    "# plt.title('Average Reward vs Episode with Smoothed Rewards')\n",
    "# plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "plt.tight_layout()\n",
    "save_results_to = 'C:/Users/Zunayeed/Desktop/PhD/WICL/NSF-AoF Project/Simulation BC EH/New Plots/Throughput/'\n",
    "plt.savefig(save_results_to + 'Avg_Reward_400S_600E.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bbd217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of steps each episode will have. 1 step = 1 time slot\n",
    "nSteps = 200 #number of time steps, in each time step, AP chooses a new channel ## 1 time step = 1 time slot\n",
    "\n",
    "#training parameters\n",
    "#In this case, number of steps per episode = number of time slots per episode\n",
    "#No terminating condition - each episode run up to \"nSteps\" - time slots\n",
    "num_episodes = 600 #total no. of episodes the agent will play during training\n",
    "max_steps_per_episode = nSteps #one of the terminating condition, max no. steps in a single episode\n",
    "\n",
    "learning_rate = 0.9 #high LR focus more on new, less on old; low LR learn nothing, use prior knowledge\n",
    "discount_rate = 0.99 #high DR focus more on distant reward, low DR focus more on immediate reward\n",
    "\n",
    "#epsilon-greedy\n",
    "exploration_rate = 1\n",
    "exploration_rate2 = 1\n",
    "exploration_rate3 = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01 \n",
    "exploration_decay_rate2 = 0.02\n",
    "exploration_decay_rate3 = 0.05\n",
    "\n",
    "\n",
    "all_epochs = []\n",
    "all_iterations = []\n",
    "all_exploration_rate = []\n",
    "all_exploration_rate2 = []\n",
    "all_exploration_rate3 = []\n",
    "rewards_all_steps = []\n",
    "avg_reward_all_episodes2 = []\n",
    "rewards_current_step = 0\n",
    "iterations = 0\n",
    "epochs = 0\n",
    "reward = np.zeros(max_steps_per_episode)\n",
    "P_R_AP = np.zeros(max_steps_per_episode)\n",
    "P_N = np.zeros(max_steps_per_episode)\n",
    "SNR = np.zeros(max_steps_per_episode)\n",
    "EB = 0\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    current_state = 0\n",
    "    epochs += 1\n",
    "    #rewards_current_step = 0\n",
    "    rewards_each_episode2 = []\n",
    "    #avg_reward = 0\n",
    "    \n",
    "    avg_reward2 = sum(rewards_each_episode2)/max_steps_per_episode\n",
    "    avg_reward_all_episodes2.append(avg_reward2)\n",
    "    # Exploration rate decay (at the end of one episode, we need to update the exploration rate)\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    exploration_rate2 = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate2*episode)\n",
    "        \n",
    "    exploration_rate3 = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate3*episode)\n",
    "    all_epochs.append(epochs)\n",
    "    all_exploration_rate.append(exploration_rate)\n",
    "    all_exploration_rate2.append(exploration_rate2)\n",
    "    all_exploration_rate3.append(exploration_rate3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc3d3152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting exploration rate over 100 episodes\n",
    "\n",
    "plt.plot(all_epochs,all_exploration_rate,'-*', label = 'Decay Rate 0.01', color = 'blue')\n",
    "plt.plot(all_epochs,all_exploration_rate2,'-*', label = 'Decay Rate 0.02', color = 'green')\n",
    "plt.plot(all_epochs,all_exploration_rate3,'-*', label = 'Decay Rate 0.05', color = 'red')\n",
    "plt.legend(['Decay Rate = 0.01', 'Decay Rate = 0.02', 'Decay Rate = 0.05'])\n",
    "\n",
    "plt.xlabel(r'Episode')\n",
    "plt.ylabel(r'Exploration Rate')\n",
    "plt.title('Exploration Rate over Multiple Episodes')\n",
    "plt.grid(True)\n",
    "\n",
    "save_results_to = 'C:/Users/Zunayeed/Desktop/PhD/WICL/NSF-AoF Project/Simulation BC EH/New Plots/'\n",
    "plt.savefig(save_results_to + 'Exploration Rate_3Rates.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05fd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
